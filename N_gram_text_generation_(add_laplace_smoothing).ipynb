{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faezesarlakifar/SBU-NLP-Lab-summer-school/blob/main/N_gram_text_generation_(add_laplace_smoothing).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6596167c",
      "metadata": {
        "id": "6596167c"
      },
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Sbu-logo.svg/1200px-Sbu-logo.svg.png\" alt=\"keras\" width=\"150\" height=\"150\">\n",
        "\n",
        "<h1 align=center><font size = 7>NLP Summer school</font></h1>\n",
        "<h1 align=center><font size = 6>NLP Research Lab</font></h1>\n",
        "<h1 align=center><font size = 5>Shahid Beheshti University</font></h1>\n",
        "<h1 align=center><font size = 4> July 2022 </font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c417e528",
      "metadata": {
        "id": "c417e528"
      },
      "source": [
        "## Original Repo\n",
        "https://github.com/olegborisovv/NGram_LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4f2db283",
      "metadata": {
        "id": "4f2db283"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import random\n",
        "import time\n",
        "from typing import List\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "903e8034",
      "metadata": {
        "id": "903e8034"
      },
      "source": [
        "#### initialize parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9a4adee8",
      "metadata": {
        "id": "9a4adee8"
      },
      "outputs": [],
      "source": [
        "n = 6\n",
        "path = 'Frankenstein.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e3513278",
      "metadata": {
        "id": "e3513278"
      },
      "outputs": [],
      "source": [
        "# ideally we would use some smart text tokenizer, but for simplicity use this one\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    :param text: Takes input sentence\n",
        "    :return: tokenized sentence\n",
        "    \"\"\"\n",
        "    for punct in string.punctuation:\n",
        "        text = text.replace(punct, ' '+punct+' ')\n",
        "    t = text.split()\n",
        "\n",
        "    return t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e8a8d02d",
      "metadata": {
        "id": "e8a8d02d"
      },
      "outputs": [],
      "source": [
        "def get_ngrams(n: int, tokens: list) -> list:\n",
        "    \"\"\"\n",
        "    :param n: n-gram size\n",
        "    :param tokens: tokenized sentence\n",
        "    :return: list of ngrams\n",
        "\n",
        "    ngrams of tuple form: ((previous wordS!), target word)\n",
        "    \"\"\"\n",
        "    # tokens.append('<END>')\n",
        "    tokens = (n-1)*['<START>']+tokens\n",
        "    l = [(tuple([tokens[i-p-1] for p in reversed(range(n-1))]), tokens[i]) for i in range(n-1, len(tokens))]\n",
        "    return l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af89f2de",
      "metadata": {
        "id": "af89f2de"
      },
      "source": [
        "#### get all tokens and size of vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "92949d2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92949d2c",
        "outputId": "934d7ae9-d7eb-47a5-8f1f-3ec49a25e10d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['was', 'often', 'tempted', ',', 'when', 'all', 'was', 'at', 'peace', 'around', 'me', ',', 'and', 'I', 'the', 'only', 'unquiet', 'thing', 'that', 'wandered', 'restless', 'in', 'a', 'scene', 'so', 'beautiful', 'and', 'heavenly—if', 'I', 'except', 'some', 'bat', ',', 'or', 'the', 'frogs', ',', 'whose', 'harsh', 'and', 'interrupted', 'croaking', 'was', 'heard', 'only', 'when', 'I', 'approached', 'the', 'shore—often', ',', 'I', 'say', ',', 'I', 'was', 'tempted', 'to', 'plunge', 'into', 'the', 'silent', 'lake', ',', 'that', 'the', 'waters', 'might', 'close', 'over', 'me', 'and', 'my', 'calamities', 'for', 'ever', '.', 'I', 'learned', 'from', 'your', 'papers', 'that', 'you', 'were', 'my', 'father', ',', 'my', 'creator', ';', 'and', 'to', 'whom', 'could', 'I', 'apply', 'with', 'more']\n",
            "6408\n"
          ]
        }
      ],
      "source": [
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        text = ' '.join([line.strip() for line in f.readlines() if not line.startswith('#')])\n",
        "        text = text.split('.')\n",
        "        text = random.sample(text, 2000)\n",
        "        tokens = []\n",
        "        for sentence in text:\n",
        "            # add back the fullstop\n",
        "            sentence += '.'\n",
        "            tokens.extend(tokenize(sentence))\n",
        "\n",
        "    print(tokens[1:100])\n",
        "    vocab  = nltk.FreqDist(tokens)\n",
        "    vocab_size = len(vocab)\n",
        "    print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7754c5a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7754c5a6",
        "outputId": "fb4df14b-6e3c-4233-b2a7-1c0468862d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57807\n"
          ]
        }
      ],
      "source": [
        "n_grams = nltk.ngrams(tokens, n)\n",
        "n_vocab = nltk.FreqDist(n_grams)\n",
        "\n",
        "m = n - 1\n",
        "m_grams = nltk.ngrams(tokens, m)\n",
        "m_vocab = nltk.FreqDist(m_grams)\n",
        "\n",
        "print(len(m_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e7db58d5",
      "metadata": {
        "id": "e7db58d5"
      },
      "outputs": [],
      "source": [
        "class NgramModel(object):\n",
        "\n",
        "    def __init__(self, n, laplace=1):\n",
        "        self.n = n\n",
        "\n",
        "        # dictionary that keeps list of candidate words given context\n",
        "        self.context = {}\n",
        "\n",
        "        # keeps track of how many times ngram has appeared in the text before\n",
        "        self.ngram_counter = {}\n",
        "\n",
        "        # use this for  Laplace smoothing to n-gram frequency distribution\n",
        "        self.laplace = laplace\n",
        "\n",
        "        # use vocab size in laplace smoothing and we update its value later\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def update(self, sentence: str) -> None:\n",
        "        \"\"\"\n",
        "        Updates Language Model\n",
        "        :param sentence: input text\n",
        "        \"\"\"\n",
        "        n = self.n\n",
        "        ngrams = get_ngrams(n, tokenize(sentence))\n",
        "        for ngram in ngrams:\n",
        "            if ngram in self.ngram_counter:\n",
        "                self.ngram_counter[ngram] += 1.0\n",
        "            else:\n",
        "                self.ngram_counter[ngram] = 1.0\n",
        "\n",
        "            prev_words, target_word = ngram\n",
        "            if prev_words in self.context:\n",
        "                self.context[prev_words].append(target_word)\n",
        "            else:\n",
        "                self.context[prev_words] = [target_word]\n",
        "\n",
        "    def smooth(self, token, count_of_token, count_of_context):\n",
        "\n",
        "        count = count_of_token\n",
        "\n",
        "        def smoothed_count(n_gram, n_count):\n",
        "            m_gram = n_gram[:-1]\n",
        "            m_count = m_vocab[token]\n",
        "            if(m_count == 0):\n",
        "                m_count = count_of_context\n",
        "            return (n_count + self.laplace) / (m_count + (self.laplace * self.vocab_size))\n",
        "\n",
        "        return smoothed_count(token, count)\n",
        "\n",
        "    def prob(self, context, token):\n",
        "        \"\"\"\n",
        "        Calculates probability of a candidate token to be generated given a context\n",
        "        :return: conditional probability\n",
        "        \"\"\"\n",
        "        count_of_token = self.ngram_counter[(context, token)]\n",
        "        count_of_context = float(len(self.context[context]))\n",
        "\n",
        "        \"\"\"\n",
        "        that exception occurred when context size is 0 and we have divide by zero exception.\n",
        "        we use laplace smmoothing for handle this problem.\n",
        "        \"\"\"\n",
        "\n",
        "        if (self.n == 1):\n",
        "            x = count_of_token + self.laplace\n",
        "            y = count_of_context + (self.laplace * vocab_size)\n",
        "            result = x / y\n",
        "        else:\n",
        "            result = self.smooth(token, count_of_token, count_of_context)\n",
        "        return result\n",
        "\n",
        "    def random_token(self, context):\n",
        "        \"\"\"\n",
        "        Given a context we \"semi-randomly\" select the next word to append in a sequence\n",
        "        :param context:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        r = random.random()\n",
        "        map_to_probs = {}\n",
        "        try:\n",
        "            token_of_interest = self.context[context]\n",
        "            for token in token_of_interest:\n",
        "                map_to_probs[token] = self.prob(context, token)\n",
        "\n",
        "            summ = 0\n",
        "            for token in sorted(map_to_probs):\n",
        "#             return token\n",
        "                summ += map_to_probs[token]\n",
        "                if summ > r:\n",
        "                    return token\n",
        "        except KeyError:\n",
        "            pass\n",
        "            #return random.choice(str(tokens))\n",
        "\n",
        "    def generate_text(self, token_count: int):\n",
        "        \"\"\"\n",
        "        :param token_count: number of words to be produced\n",
        "        :return: generated text\n",
        "        \"\"\"\n",
        "        n = self.n\n",
        "        context_queue = (n - 1) * ['<START>']\n",
        "        result = []\n",
        "        for _ in range(token_count):\n",
        "            obj = self.random_token(tuple(context_queue))\n",
        "            result.append(obj)\n",
        "            if n > 1:\n",
        "                context_queue.pop(0)\n",
        "                if (obj == '.'):\n",
        "                    context_queue = (n - 1) * ['<START>']\n",
        "                else:\n",
        "                    context_queue.append(obj)\n",
        "        return ' '.join(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61312c3f",
      "metadata": {
        "id": "61312c3f"
      },
      "source": [
        "How can we add smoothing functionality?\n",
        "\n",
        "Why didn't we return the most probable token? (stay tuned for the rest of the materials)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "543fd79f",
      "metadata": {
        "id": "543fd79f"
      },
      "outputs": [],
      "source": [
        "def create_ngram_model(n, path):\n",
        "    m = NgramModel(n)\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        text = ' '.join([line.strip() for line in f.readlines() if not line.startswith('#')])\n",
        "        text = text.split('.')\n",
        "        text = random.sample(text, 2000)\n",
        "        for sentence in text:\n",
        "            # add back the fullstop\n",
        "            sentence += '.'\n",
        "            m.update(sentence)\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "36c6de0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36c6de0f",
        "outputId": "a0e639b8-6dba-4901-8bd4-b35ab603b033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language Model creating time: 0.33425021171569824\n",
            "========================================================================================================================\n",
            "Generated text:\n",
            "It is impossible to communicate to you a conception of the trembling sensation , half pleasurable and half fearful , with which I am preparing to depart . Often , when all was dry , the heavens cloudless , and I was parched by thirst , a slight cloud would bedim the sky , shed the few drops that revived me , and vanish . Oppressed by the recollection of my various misfortunes , I now swallowed double my usual quantity and soon slept profoundly . Will you smile at the enthusiasm I express concerning this divine wanderer ? You\n",
            "========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    m = create_ngram_model(6, 'Frankenstein.txt')\n",
        "\n",
        "    print (f'Language Model creating time: {time.time() - start}')\n",
        "    start = time.time()\n",
        "#     random.seed(44)\n",
        "    print(f'{\"=\"*120}\\nGenerated text:')\n",
        "    print(m.generate_text(100))\n",
        "    print(f'{\"=\"*120}')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df7fa69",
      "metadata": {
        "id": "8df7fa69"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}