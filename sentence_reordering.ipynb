{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "5d9c6aec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d9c6aec",
        "outputId": "ca9f2aed-0074-479f-ac19-c3c185cee429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "import csv \n",
        "!pip install hazm\n",
        "import hazm\n",
        "from hazm import Normalizer, sent_tokenize, word_tokenize \n",
        "import string\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset \n",
        "import gensim\n",
        "import logging\n",
        "import os\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n",
        "from time import time \n",
        "!pip install transformers\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, AdamW\n",
        "import numpy as np\n",
        "import torch.utils.data as data_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "vI7__pJek6Z3",
      "metadata": {
        "id": "vI7__pJek6Z3"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "94e4b0d5",
      "metadata": {
        "id": "94e4b0d5"
      },
      "outputs": [],
      "source": [
        "path2 = 'persian-sentences-dataset-1.csv' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a6330e85",
      "metadata": {
        "id": "a6330e85"
      },
      "outputs": [],
      "source": [
        "path = 'Persian-WikiText-1.txt' "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f2ced3",
      "metadata": {
        "id": "f6f2ced3"
      },
      "source": [
        "### Sentence Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "905b26e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "905b26e8",
        "outputId": "31ae4312-d7c0-49d5-9c09-e3845a2106e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['', 'سلام', ' خوبی', ' می دونم سرت شلوغه', ' باشه', ' ولی بیا این طرفا', ' باشه ممنونم', ' تو خوبی', ' آره شکر، خوبم']\n"
          ]
        }
      ],
      "source": [
        "def sentence_tokenizer(text):\n",
        "    text = text.split('.')\n",
        "    sentences = [] \n",
        "    for sentence in text: \n",
        "        if '!' in sentence:\n",
        "            sentence = sentence.split('!')\n",
        "            for subsen in sentence:\n",
        "                if '؟' in subsen:\n",
        "                    subsen2 = subsen.split('؟')\n",
        "                    sentences.extend(subsen2)\n",
        "                else:\n",
        "                    sentences.append(subsen)\n",
        "        elif '؟' in sentence:\n",
        "            sentence_ = sentence.split('؟')\n",
        "            sentences.extend(sentence_)\n",
        "        else:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "text = '.سلام! خوبی؟ می دونم سرت شلوغه. باشه! ولی بیا این طرفا. باشه ممنونم. تو خوبی؟ آره شکر، خوبم'\n",
        "sentences2 = sentence_tokenizer(text)\n",
        "print(sentences2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "5ea3cc13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ea3cc13",
        "outputId": "4fb63470-53e7-43d6-c9d2-b9585acea4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ویکی پدیا (کوته نوشت به صورت «وپ» و «WP») یک دانشنامه برخط چندزبانه مبتنی بر وب با محتوای آزاد و همکاری باز است که با همکاری افراد داوطلب نوشته می‌شود و هر کسی که به اینترنت و وب دسترسی داشته باشد می‌تواند مقالات آن را ببیند و ویرایش کند\n"
          ]
        }
      ],
      "source": [
        "with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "      text = ' '.join([line.strip() for line in f.readlines() if not (('*' in line) or ('عنوان مقاله' in line))])\n",
        "        \n",
        "      # preprocess data\n",
        "      normalizer = Normalizer()\n",
        "      text = normalizer.normalize(text)\n",
        "      sentences = sentence_tokenizer(text) \n",
        "                            \n",
        "print(sentences[0])     \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "533e42c1",
      "metadata": {
        "id": "533e42c1"
      },
      "source": [
        "### Word Embeding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1e395b21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e395b21",
        "outputId": "65d1a102-50d6-4062-d5e6-f5d8c44c85c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "config = AutoConfig.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "jW4r5lJs-cJC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW4r5lJs-cJC",
        "outputId": "a3757cb5-1b66-4fbd-db54-be5939f9aebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['سلام', 'ما', 'که', 'رفتیم', 'ولی', 'شما', 'بمونید']\n"
          ]
        }
      ],
      "source": [
        "def remove_punctuation(txt):\n",
        "    s = \"\"\n",
        "    punc = string.punctuation+'،؛«»'\n",
        "    for l in txt:\n",
        "        if l not in punc:\n",
        "            s+=l\n",
        "    return s\n",
        "\n",
        "def my_word_tokenizer(txt):\n",
        "    txt = remove_punctuation(txt)\n",
        "    doc = word_tokenize(txt)\n",
        "    return doc\n",
        "\n",
        "txt = 'سلام، ما که رفتیم؛ ولی شما بمونید.'\n",
        "words = my_word_tokenizer(txt)\n",
        "print(words) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "ihv4izW7sdFU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihv4izW7sdFU",
        "outputId": "a74103ad-531a-4eb4-9ea5-e3e9d96ba7f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "سلام          3,132\n",
            "دوست          3,584\n",
            "خوبم         35,065\n",
            "ما            2,179\n",
            "داریم         3,194\n",
            "میریم        49,824\n",
            "مدرسه         5,000\n"
          ]
        }
      ],
      "source": [
        "marked_text = 'سلام دوست خوبم ما داریم میریم مدرسه'\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indeces.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e414065",
      "metadata": {
        "id": "6e414065"
      },
      "source": [
        "### Sentences Shuffling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3f634b70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f634b70",
        "outputId": "ae8afdf5-f5a0-454d-c9fd-5fd72d0c6c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['نمیام نه من', 'بریم بیا کافه']\n"
          ]
        }
      ],
      "source": [
        "def prepare_shuffled_sentences(sentences):\n",
        "    shuffled_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence_words = my_word_tokenizer(sentence)\n",
        "        random.shuffle(sentence_words)\n",
        "        shuffled = ' '.join(sentence_words)\n",
        "        shuffled_sentences.append(shuffled)\n",
        "    return shuffled_sentences\n",
        "\n",
        "sentences_2 = ['نه من نمیام.', '!بیا بریم کافه']\n",
        "shuffled_sentences_2 = prepare_shuffled_sentences(sentences_2)\n",
        "print(shuffled_sentences_2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9736ebdc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9736ebdc",
        "outputId": "cd81f2cf-8b73-438c-e81a-9ec8b22fd921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "می‌تواند مبتنی با نوشت است و آن برخط و دسترسی کوته آزاد ویرایش داوطلب داشته_باشد نوشته_می‌شود اینترنت صورت ببیند ویکی WP بر همکاری به محتوای با مقالات دانشنامه باز افراد و وب به کند وپ وب چندزبانه هر را که پدیا یک و و کسی همکاری که\n"
          ]
        }
      ],
      "source": [
        "shuffled_sentences = prepare_shuffled_sentences(sentences)\n",
        "print(shuffled_sentences[0]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "B82MZXvHLf5j",
      "metadata": {
        "id": "B82MZXvHLf5j"
      },
      "outputs": [],
      "source": [
        "def get_bert_input(text, tokenizer, max_len=512):\n",
        "   \n",
        "    cls_token = '[CLS]'\n",
        "    sep_token = '[SEP]'\n",
        "    word_piece_list = tokenizer.tokenize(text) \n",
        "    if len(word_piece_list) > max_len-2: # notice \n",
        "        word_piece_list = word_piece_list[:510]\n",
        "    word_piece_list.insert(0, cls_token)\n",
        "    word_piece_list.append(sep_token)\n",
        "    \n",
        "    input_ids = my_word_tokenizer(word_piece_list) \n",
        "      \n",
        "    input_mask  = []\n",
        "    for i in range(len(input_ids)):\n",
        "        input_mask.append(1) \n",
        "    while len(input_mask ) < max_len:\n",
        "        input_mask.append(0)\n",
        "    while len(input_ids) < max_len:\n",
        "        input_ids.append(0)\n",
        "    segment_ids = [0] * max_len\n",
        "    return input_ids, input_mask, segment_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ew9JBaW8zRDp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew9JBaW8zRDp",
        "outputId": "3b857647-405d-4dff-f313-900d953c293c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[13827, 39512, 31487, 3695, 2031, 2353, 36562, 331, 1, 2076, 19725, 18349, 1, 6184, 2043, 4068, 2037, 6693, 1, 331, 2886, 2266, 2045, 2046, 2037, 2886, 2698, 7409, 1, 331, 2202, 3256, 2046, 2031, 3975, 331, 4068, 4405, 1, 1, 9792, 1, 2049, 9737, 331, 10077, 2299]\n",
            "[9792, 1, 331, 13827, 2037, 2046, 4068, 1, 331, 2049, 19725, 2031, 2886, 2043, 4405, 3975, 4068, 331, 331, 1, 2037, 2031, 2045, 7409, 1, 2299, 39512, 1, 6693, 2886, 2698, 10077, 331, 2076, 3695, 2353, 9737, 31487, 3256, 2202, 6184, 2266, 1, 18349, 36562, 1, 2046]\n",
            "[13827, 39512, 31487, 3695, 2031, 2353, 36562, 331, 1, 2076, 19725, 18349, 1, 6184, 2043, 4068, 2037, 6693, 1, 331, 2886, 2266, 2045, 2046, 2037, 2886, 2698, 7409, 1, 331, 2202, 3256, 2046, 2031, 3975, 331, 4068, 4405, 1, 1, 9792, 1, 2049, 9737, 331, 10077, 2299]\n"
          ]
        }
      ],
      "source": [
        "target_vectors = []\n",
        "input_vectors = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence_words = my_word_tokenizer(sentence)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(sentence_words)\n",
        "    if len(indexed_tokens) > 60:\n",
        "        continue\n",
        "    target_vectors.append(indexed_tokens)\n",
        "\n",
        "print(target_vectors[0])\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence_words = my_word_tokenizer(sentence)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(sentence_words)\n",
        "    if len(indexed_tokens) > 60:\n",
        "        continue\n",
        "    random.shuffle(indexed_tokens)\n",
        "    input_vectors.append(indexed_tokens)\n",
        "\n",
        "\n",
        "print(input_vectors[0])\n",
        "print(target_vectors[0])\n",
        "\n",
        "inputs = input_vectors\n",
        "targets = target_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0073467b",
      "metadata": {
        "id": "0073467b"
      },
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "9073b6e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9073b6e0",
        "outputId": "2d44ddde-53fa-4272-a935-08202bb6f312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "443174\n"
          ]
        }
      ],
      "source": [
        "# show the size of data\n",
        "print(len(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8141496",
      "metadata": {
        "id": "b8141496"
      },
      "source": [
        "### split data into train and test parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "a9dd468c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9dd468c",
        "outputId": "12a5a2a6-39e0-4cfc-d9c0-4f495f5e0589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of train data is: 353173\n",
            "size of test data is: 90000\n"
          ]
        }
      ],
      "source": [
        "train_X_data = inputs[90001:]\n",
        "train_Y_data = targets[90001:]\n",
        "test_X_data = inputs[:90000]\n",
        "test_Y_data = targets[:90000]\n",
        "\n",
        "print(\"size of train data is:\", len(train_X_data))\n",
        "print(\"size of test data is:\", len(test_X_data)) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q8jRbSMh0rsQ",
      "metadata": {
        "id": "q8jRbSMh0rsQ"
      },
      "source": [
        "### Convert word embedings to Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6gTCfQq_AqGh",
      "metadata": {
        "id": "6gTCfQq_AqGh"
      },
      "outputs": [],
      "source": [
        "def zero_padding(l, width):\n",
        "    l.extend([0] * (width - len(l)))\n",
        "    return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "3jNshcTmBkUk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jNshcTmBkUk",
        "outputId": "b52dd5f7-1855-45a3-a25c-e4f11739fa90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "def FindMaxLength(lst):\n",
        "    list_len = []\n",
        "    len_ = 0\n",
        "    for item in lst:\n",
        "        for x in item:\n",
        "            len_ += 1\n",
        "        list_len.append(len_)\n",
        "        len_ = 0\n",
        "    maxLength = max(list_len)\n",
        "    return maxLength\n",
        "\n",
        "\n",
        "my_lst = [[1, 3, 6, 7, 8, 90], [1, 2], [0], [15, 8, 9]]\n",
        "max1 = FindMaxLength(my_lst)\n",
        "print(max1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "8cCTb7MTF1lX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cCTb7MTF1lX",
        "outputId": "664b9748-02cf-4290-9776-f59940b6aaa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9792, 1, 331, 13827, 2037, 2046, 4068, 1, 331, 2049, 19725, 2031, 2886, 2043, 4405, 3975, 4068, 331, 331, 1, 2037, 2031, 2045, 7409, 1, 2299, 39512, 1, 6693, 2886, 2698, 10077, 331, 2076, 3695, 2353, 9737, 31487, 3256, 2202, 6184, 2266, 1, 18349, 36562, 1, 2046]\n",
            "[13827, 39512, 31487, 3695, 2031, 2353, 36562, 331, 1, 2076, 19725, 18349, 1, 6184, 2043, 4068, 2037, 6693, 1, 331, 2886, 2266, 2045, 2046, 2037, 2886, 2698, 7409, 1, 331, 2202, 3256, 2046, 2031, 3975, 331, 4068, 4405, 1, 1, 9792, 1, 2049, 9737, 331, 10077, 2299]\n",
            "max length of train sentence is: 60\n",
            "max length of test sentence is: 60\n"
          ]
        }
      ],
      "source": [
        "max_train = FindMaxLength(train_X_data)\n",
        "max_test  = FindMaxLength(test_X_data)\n",
        "\n",
        "print(test_X_data[0])\n",
        "print(test_Y_data[0])\n",
        "print('max length of train sentence is:', max_train)\n",
        "print('max length of test sentence is:', max_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m1IPhxi-WHH2",
      "metadata": {
        "id": "m1IPhxi-WHH2"
      },
      "source": [
        "- Add zero padding to be compatible with Tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "7Z6bvRmcWWcq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z6bvRmcWWcq",
        "outputId": "33db9c66-301b-4f8e-e150-e54f7c2578e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3130, 2028, 16889, 2042, 2297, 2083, 6036, 2116, 2036, 9170, 331, 10361, 19987, 2813, 7042, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[2042, 2116, 2297, 2036, 19987, 3130, 7042, 9170, 2083, 331, 2028, 16889, 2813, 10361, 6036, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "train_X_with_zero_padding = []\n",
        "train_Y_with_zero_padding = []\n",
        "for i in range(len(train_X_data)):\n",
        "    train_X_with_zero_padding.append(zero_padding(train_X_data[i], 60))\n",
        "    train_Y_with_zero_padding.append(zero_padding(train_Y_data[i], 60))\n",
        "\n",
        "train_X_data = train_X_with_zero_padding\n",
        "train_Y_data = train_Y_with_zero_padding\n",
        "\n",
        "print(train_X_data[0])\n",
        "print(train_Y_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "5P583MHxY7C3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P583MHxY7C3",
        "outputId": "6ffed3d0-b53b-465c-9cc1-e8d2d10bbc61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60\n"
          ]
        }
      ],
      "source": [
        "print(FindMaxLength(train_Y_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c8UdAfb1Xx8y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8UdAfb1Xx8y",
        "outputId": "38efbdac-5a0f-495f-f3ce-8ce61b698465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9792, 1, 331, 13827, 2037, 2046, 4068, 1, 331, 2049, 19725, 2031, 2886, 2043, 4405, 3975, 4068, 331, 331, 1, 2037, 2031, 2045, 7409, 1, 2299, 39512, 1, 6693, 2886, 2698, 10077, 331, 2076, 3695, 2353, 9737, 31487, 3256, 2202, 6184, 2266, 1, 18349, 36562, 1, 2046, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[13827, 39512, 31487, 3695, 2031, 2353, 36562, 331, 1, 2076, 19725, 18349, 1, 6184, 2043, 4068, 2037, 6693, 1, 331, 2886, 2266, 2045, 2046, 2037, 2886, 2698, 7409, 1, 331, 2202, 3256, 2046, 2031, 3975, 331, 4068, 4405, 1, 1, 9792, 1, 2049, 9737, 331, 10077, 2299, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "test_X_with_zero_padding = []\n",
        "test_Y_with_zero_padding = []\n",
        "for i in range(len(test_X_data)):\n",
        "    test_X_with_zero_padding.append(zero_padding(test_X_data[i], 60))\n",
        "    test_Y_with_zero_padding.append(zero_padding(test_Y_data[i], 60))\n",
        "\n",
        "test_X_data = test_X_with_zero_padding\n",
        "test_Y_data = test_Y_with_zero_padding\n",
        "\n",
        "print(test_X_data[0])\n",
        "print(test_Y_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "xo0shBSq0ieJ",
      "metadata": {
        "id": "xo0shBSq0ieJ"
      },
      "outputs": [],
      "source": [
        "train_X = torch.cuda.FloatTensor([train_X_data])\n",
        "train_Y = torch.cuda.FloatTensor([train_Y_data])\n",
        "test_X = torch.cuda.FloatTensor([test_X_data])\n",
        "test_Y = torch.cuda.FloatTensor([test_Y_data]) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "snWPRzdQnusK",
      "metadata": {
        "id": "snWPRzdQnusK"
      },
      "source": [
        "### Passing to DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "LBZBWPRW0gxd",
      "metadata": {
        "id": "LBZBWPRW0gxd"
      },
      "outputs": [],
      "source": [
        "train = data_utils.TensorDataset(train_X, train_Y)\n",
        "train_loader = data_utils.DataLoader(train, batch_size=32, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(test_X, test_Y)\n",
        "test_loader = data_utils.DataLoader(test, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n14K8UCz1Iwt",
      "metadata": {
        "id": "n14K8UCz1Iwt"
      },
      "source": [
        "### Set Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "GYocPkRY1ETk",
      "metadata": {
        "id": "GYocPkRY1ETk"
      },
      "outputs": [],
      "source": [
        "epochs = 70\n",
        "batch_size = 32\n",
        "learning_rate = 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ZCoNB2k1QZr",
      "metadata": {
        "id": "8ZCoNB2k1QZr"
      },
      "source": [
        "### Define OPtimizer and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "IVxildbM1Dxi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVxildbM1Dxi",
        "outputId": "4e26078c-95ad-47ec-95e9-de45b7d16bca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr = learning_rate, eps = 1e-8)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pwW0Xl-g7pzS",
      "metadata": {
        "id": "pwW0Xl-g7pzS"
      },
      "outputs": [],
      "source": [
        "for name, layer in model.named_modules():\n",
        "       print(name, layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074EsPqO_EVY",
      "metadata": {
        "id": "074EsPqO_EVY"
      },
      "source": [
        "### Not Trainig all layers again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "SNtYIou02MRZ",
      "metadata": {
        "id": "SNtYIou02MRZ"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters(): \n",
        "    param.requires_grad = False \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RmXX2Ipw_I3A",
      "metadata": {
        "id": "RmXX2Ipw_I3A"
      },
      "source": [
        "### But train the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "Ls8RXZYC2LwS",
      "metadata": {
        "id": "Ls8RXZYC2LwS"
      },
      "outputs": [],
      "source": [
        "for param in model.encoder.layer[11].parameters():\n",
        "    param.requires_grad = True "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PegVqMe_1t3x",
      "metadata": {
        "id": "PegVqMe_1t3x"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "E4BXFnBhF7Fo",
      "metadata": {
        "id": "E4BXFnBhF7Fo"
      },
      "outputs": [],
      "source": [
        "indexes = []\n",
        "ctr = 0\n",
        "for element in train_X:\n",
        "    indexes.append(ctr)\n",
        "    ctr += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "UAJ3XmLmB7zs",
      "metadata": {
        "id": "UAJ3XmLmB7zs"
      },
      "outputs": [],
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "odgBoELv1akX",
      "metadata": {
        "id": "odgBoELv1akX"
      },
      "outputs": [],
      "source": [
        "model.train() \n",
        "\n",
        "loss_values = [] \n",
        "running_loss = 0.0\n",
        "for epoch in range(epochs): \n",
        "    for i, (inp, out) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        out = out.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(inputs_embeds=inp, output_hidden_states=False, return_dict=True)\n",
        "\n",
        "        loss = criterion(outputs, out)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        loss_values.append(loss.item())\n",
        "        \n",
        "    print('Epoch [{}/{}]: Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yg9B5oJgoNtO",
      "metadata": {
        "id": "yg9B5oJgoNtO"
      },
      "source": [
        "### Save DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8efExbbmoPlI",
      "metadata": {
        "id": "8efExbbmoPlI"
      },
      "outputs": [],
      "source": [
        "torch.save(train_loader, 'train_loader.pth')\n",
        "torch.save(test_loader, 'test_loader.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "sentence_reordering.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
