{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faezesarlakifar/SBU-NLP-Lab-summer-school/blob/main/seq2seq_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY5r2-93D84W"
      },
      "source": [
        "# Sequence to Sequence Learning with Neural Network\n",
        "\n",
        "Acknowledgement : this notebook origins from https://github.com/bentrevett/pytorch-seq2seq\n",
        "\n",
        "Note : This notebook is just for learning Seq2seq model.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch==1.8.0 torchtext==0.9.0\n",
        "\n",
        "# Reload environment\n",
        "exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlc3TPvMaUK",
        "outputId": "f5c3d88c-5cf0-4c69-a0d4-95858d692229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "TEXT = data.Field()\n",
        "LABEL = data.LabelField(dtype = torch.long)\n",
        "legacy_train, legacy_test = datasets.IMDB.splits(TEXT, LABEL)  # datasets here refers to torchtext.legacy.datasets"
      ],
      "metadata": {
        "id": "DZxXnis1MXte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25nsH68hD84a"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "import time, random, math, string\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uv81_g5D84c"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will start simple model to understand the general concepts by implementing the model from the [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) paper.\n",
        "\n",
        "The most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which (commonly) use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector (as an abstract representation of the entrie input sentence).\n",
        "\n",
        "This vector is then decoded by a second RNN which learns to output the target(output) sentence by generating it one word at a time.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/3a8dc5515ff28cb059532439c5687126dd30015f/assets/seq2seq1.png)\n",
        "Above image shows an example translation. The input sentence \"guten morgen\", is input into the encoder (green) one word at a time. We also append a start of sequence(<sos\\>) and end of sequence(<eos\\>) token to the start and end of sentence, respectively. At each time-step, the input to the encoder RNN is both the current word $x_t$ as well as the hidden state from the previous time-step $h_{t-1}$ You can think of the hidden state as a vector representation of the sentence so far. The RNN can be represented as a function of both $x_t$ and $h_{t-1}$ :\n",
        "$$\n",
        "h_t = EncoderRNN(x_t, h_{t-1}) \\tag{1}\n",
        "$$\n",
        "Here, we have $X={x_1, x_2, \\cdots, x_T}$ where $x_1$ = <sos\\> $x_2$ = guten, etc. The initial hidden states $h_0$ is usually either initialized to zeros or a learned parameter.\n",
        "\n",
        "Once the final word $x_T$ has been passed into RNN, we use the final hidden state $h_T$ as the context vector i.e. $h_T = z$.\n",
        "\n",
        "With our context vector $z$, we can start decoding it to get the target sentence, \"good morning\". Again we append start and end of sequence tokens to the target sentence. At each time-step, the input to the decoder RNN (blue) is the current word, $y_t$, as well as the hidden state from the previous time-step $s_{t-1}$, where the initail decoder hidden state $s_0 = z = h_T$ i.e. the initial hidden state is the final encoder hidden state. similar to the encoder, we can represent the decoder as:\n",
        "$$\n",
        "s_t = DecoderRNN(y_t, s_{t-1}) \\tag{2}\n",
        "$$\n",
        "In the decoder, we need to go from the hidden state to an actual word, therefore at each time-step we use $s_t$ to predict (by passing it through a Linear layer, shown in purple) what we think is the next word in the sequence $\\hat{y}_t$.\n",
        "$$\n",
        "\\hat{y}_t = f(s_t) \\tag{3}\n",
        "$$\n",
        "The word in the encoder are always generated one after another, with one per time-step. We always use the <sos\\> for the first input to the decoder $y_1$, but for subsequent inputs $y_{t > 1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder $\\hat{y}_{t-1}$. This is called teacher forcing.\n",
        "\n",
        "When training/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference (i.e. real world usage) it is common to keep generating words until the model outputs an <eos\\> token or after a certain amount of words have been generated.\n",
        "\n",
        "Once we get our prediction $\\hat{Y} = {\\hat{y_1},\\hat{y_2},\\cdots, \\hat{y_T}}$, we compare it against our actual target sentence $Y = {y_1, y_2, \\cdots y_T}$, to calculate our loss. We then use this loss to update all of the parameters in our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JuQSwS4D84e"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "E07RDMVhD84f"
      },
      "outputs": [],
      "source": [
        "tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split()\n",
        "reverse_tokenizer = lambda x: tokenizer(x)[::-1]\n",
        "\n",
        "SRC = Field(tokenize=reverse_tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\n",
        "TRG = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n",
        "                                                   fields=(SRC, TRG))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhsTNLkcD84i",
        "outputId": "6b50de38-661d-44e8-a37b-c9e873239a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of test examples: 1000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of test examples: {len(test_data.examples)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIzFMOPFD84j",
        "outputId": "e02c58d9-0fdd-4612-ae47-2d531654551b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'src': ['büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes']}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KZuVL1lD84j",
        "outputId": "3941e79e-7a0f-411b-f2bd-ec1b1052497e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in source (de) vocabulary: 7805\n",
            "Unique tokens in target (en) vocabulary: 5940\n"
          ]
        }
      ],
      "source": [
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)\n",
        "\n",
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNbYwy9lD84k"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATHC_SIZE = 128\n",
        "# We use a BucketIterator instead of the standard Iterator as it create batches in such a way that it minimizes the amount\n",
        "# of padding in both the source and target sentences.\n",
        "train_iter, valid_iter, test_iter = BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                                          batch_size=BATHC_SIZE, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_iter:\n",
        "\n",
        "  # Let's check batch size.\n",
        "  print(batch)\n",
        "  break\n",
        "  print(batch.src[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIDAIJBaIZxi",
        "outputId": "20077ce0-4af9-49f2-86c2-31c7e26d9ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 128 from MULTI30K]\n",
            "\t[.src]:[torch.LongTensor of size 30x128]\n",
            "\t[.trg]:[torch.LongTensor of size 34x128]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Bert Tokenizer and Model"
      ],
      "metadata": {
        "id": "yP7Z9q2Z9oT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RwrMyso9m_I",
        "outputId": "0de19686-1203-41ee-af9a-9d595784a0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check output of nn.Embeding shape"
      ],
      "metadata": {
        "id": "3h0jlIMWFZxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# an Embedding module containing 10 tensors of size 3\n",
        "embedding = nn.Embedding(10, 3)\n",
        "# a batch of 2 samples of 4 indices each\n",
        "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
        "print(embedding(input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFmxwodZFhWN",
        "outputId": "60a50895-b6d9-4909-d643-8909d052741d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.1132, -1.2442,  2.1493],\n",
            "         [-0.9966,  1.7183, -1.1258],\n",
            "         [-0.4949, -0.8033, -0.1762],\n",
            "         [-1.4038,  0.7248, -0.0567]],\n",
            "\n",
            "        [[-0.4949, -0.8033, -0.1762],\n",
            "         [ 0.4174,  0.0824, -0.7034],\n",
            "         [-0.9966,  1.7183, -1.1258],\n",
            "         [-0.6664, -0.4560,  0.7699]]], grad_fn=<EmbeddingBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Bert Tokenization and word ids"
      ],
      "metadata": {
        "id": "52AX5paDGB10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a test sentence.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "print (tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Y2IFfEGJI-",
        "outputId": "22e8e023-3f8b-49ea-f92e-ebdaa2617474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'this', 'is', 'a', 'test', 'sentence', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "for tup in zip(tokenized_text, test_token_ids):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj9arZGZGObH",
        "outputId": "d97c020b-3181-4a69-e3d9-b2e662ff195b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]           101\n",
            "this          2,023\n",
            "is            2,003\n",
            "a             1,037\n",
            "test          3,231\n",
            "sentence      6,251\n",
            ".             1,012\n",
            "[SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Bert Embeding Functions"
      ],
      "metadata": {
        "id": "_tG77pnxGefC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = bert_model"
      ],
      "metadata": {
        "id": "2_hyRFQ-2WRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_text_preparation(text, tokenizer):\n",
        "\n",
        "    print(text)\n",
        "\n",
        "    temp = text\n",
        "\n",
        "    marked_text = \"[CLS] \" + temp + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1]*len(indexed_tokens)\n",
        "\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "    return tokenized_text, tokens_tensor, segments_tensors"
      ],
      "metadata": {
        "id": "-PNa2ZV7es6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Bert_Embeddings(tokens_tensor, segments_tensors, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print(1)\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        # Removing the first hidden state\n",
        "        # The first state is the input state\n",
        "        print(2)\n",
        "        hidden_states = outputs[1][1:]\n",
        "        print(3)\n",
        "\n",
        "    # Getting embeddings from the final BERT layer\n",
        "    token_embeddings = hidden_states[-1]\n",
        "    print(3)\n",
        "    # Collapsing the tensor into 1-dimension\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
        "    print(4)\n",
        "    # Converting torchtensors to lists\n",
        "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
        "    print(5)\n",
        "\n",
        "    return list_token_embeddings\n",
        "    #return token_embeddings"
      ],
      "metadata": {
        "id": "hUfcNvQUf_o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Get_Batch_Bert_Embeddings(texts):\n",
        "    target_sentences_embeddings = []\n",
        "\n",
        "    for text_ in texts:\n",
        "        tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text_, tokenizer)\n",
        "        print(\"*_*\")\n",
        "        print(\"tokens_tensor shape:\", tokens_tensor.size())\n",
        "        print(\"segments_tensors shape:\", segments_tensors.size())\n",
        "        list_token_embeddings = Bert_Embeddings(tokens_tensor, segments_tensors, model)\n",
        "        print(\"**_**\")\n",
        "\n",
        "        target_sentences_embeddings.append(list_token_embeddings)\n",
        "\n",
        "    # to tensor\n",
        "    target_sentences_embeddings_tensor = torch.tensor(target_sentences_embeddings)\n",
        "\n",
        "    return target_sentences_embeddings, target_sentences_embeddings_tensor"
      ],
      "metadata": {
        "id": "CuDGSaixoixd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Bert_ids_from_texts(texts):\n",
        "    token_ids = []\n",
        "\n",
        "    for text in texts:\n",
        "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "        tokenized_text = tokenizer.tokenize(marked_text)\n",
        "        text_token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "        token_ids.append(text_token_ids)\n",
        "\n",
        "    # to tensor\n",
        "    tokens_tensor = torch.tensor(token_ids)\n",
        "    return tokens_tensor"
      ],
      "metadata": {
        "id": "brB_4ZqUBwAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_texts_from_Bert_ids(token_ids):\n",
        "    outputs = []\n",
        "\n",
        "    for token_id in token_ids:\n",
        "        output = tokenizer.convert_ids_to_tokens(token_id)\n",
        "        print(\"output is:\", output)\n",
        "        outputs.append(output)\n",
        "\n",
        "    # to tensor\n",
        "    #tokens_tensor = torch.tensor(final_results)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "eo6zWJP2IoAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Functions"
      ],
      "metadata": {
        "id": "Jgg1WavaLLom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = ['you are my best friend!', 'my class is starting now.']\n",
        "ids = get_Bert_ids_from_texts(texts)\n",
        "print(ids)\n",
        "out = get_texts_from_Bert_ids(ids)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUrybLGFLQY-",
        "outputId": "50ee68d9-d186-4c86-a53e-456f57a2a8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 101, 2017, 2024, 2026, 2190, 2767,  999,  102],\n",
            "        [ 101, 2026, 2465, 2003, 3225, 2085, 1012,  102]])\n",
            "output is: ['[CLS]', 'you', 'are', 'my', 'best', 'friend', '!', '[SEP]']\n",
            "output is: ['[CLS]', 'my', 'class', 'is', 'starting', 'now', '.', '[SEP]']\n",
            "[['[CLS]', 'you', 'are', 'my', 'best', 'friend', '!', '[SEP]'], ['[CLS]', 'my', 'class', 'is', 'starting', 'now', '.', '[SEP]']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = ['you are my best friend', 'my class is here']\n",
        "\n",
        "\"\"\"\n",
        "for text in test_texts:\n",
        "    x, y, z = bert_text_preparation(text, tokenizer)\n",
        "\"\"\"\n",
        "embeddings, embeddings_tnesor = Get_Batch_Bert_Embeddings(test_texts)\n",
        "\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "-zeSUXijram0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7y-C7GcD84l"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "We will build our model in three parts: The encoder, the decoder, and a seq2seq model that encapsulates the encoder and decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxS9MY1aD84m"
      },
      "source": [
        "### Encoder\n",
        "First, the encoder, a 2 layer LSTM. The paper we are implementing uses a 4-layer LSTM, but in the interest of training time we cut down to 2-layers. The concept of multi-layer RNN is easy to expand from 2 to 4 layers.\n",
        "\n",
        "For a multi-layer RNN, the input sentence, $X$, goes into the first (bottom) layer of the RNN and hiddne states, $H=\\{h_1, h_2, \\cdots,h_T\\}$ output by this layer are used as inputs to the RNN in the layer above. Thus representing each layer with a superscript, the hidden states in the first layer are given by :\n",
        "$$\n",
        "h_t^1 = EncoderRNN^1(x_t, h_{t-1}^1) \\tag{4}\n",
        "$$\n",
        "The hidden states in the second layer are given by:\n",
        "$$\n",
        "h_t^2 = EncoderRNN^2(h_t^1, h_{t-1}^2) \\tag{5}\n",
        "$$\n",
        "Using a multi-layer RNN also means we'll also need an initial hidden state as input per layer, $h_0^l$, and we will also output a context vector per layer $z^l$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch6eWVgCD84m"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        \"\"\"\n",
        "        change Embedding from torch.nn.Embedding into Bert Embedding\n",
        "        \"\"\"\n",
        "        #self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src : [sen_len, batch_size]\n",
        "        print(src)\n",
        "\n",
        "        \"\"\"\n",
        "        change Embedding from torch.nn.Embedding into Bert Embedding\n",
        "        \"\"\"\n",
        "        embedded = self.dropout(Get_Batch_Bert_Embeddings(src))\n",
        "        #embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        # embedded : [sen_len, batch_size, emb_dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs = [sen_len, batch_size, hid_dim * n_directions]\n",
        "        # hidden = [n_layers * n_direction, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_direction, batch_size, hid_dim]\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZlrvyBZD84n"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "Next, we will build our decoder. which also be a 2-layer LSTM.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/3a8dc5515ff28cb059532439c5687126dd30015f/assets/seq2seq3.png)\n",
        "\n",
        "We can use the following equations to explain the decoder model.\n",
        "$$\n",
        "(s_t^1, c_t^1) = DecoderLSTM^1(y_t, (s_{t-1}^1, c_{t-1}^1))\n",
        "$$\n",
        "$$\n",
        "(s_t^2, c_t^2) = DecoderLSTM^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n",
        "$$\n",
        "Remember that the initial hidden and cell states to our decoder are our context vectors, which are the final hidden and cell of our encoder from the same layer. i.e. $(s_0^l, c_0^l) = z^l = (h_T^l, c_T^l)$.\n",
        "\n",
        "We then pass the hidden state from the top layer of the RNN, $s_t^2$ through a linear layer $f$, to make a prediction of what the next token in the target (output) sequence should be $\\hat{y}_{t+1}$\n",
        "$$\n",
        "\\hat{y}_{t+1} = f(s_t^2)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAz-NlJkD84o"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \"\"\"\n",
        "        change Embedding from torch.nn.Embedding into Bert Embedding\n",
        "        \"\"\"\n",
        "        #self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "        # input = [batch_size]\n",
        "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        # input : [1, ,batch_size]\n",
        "\n",
        "        \"\"\"\n",
        "        change Embedding from torch.nn.Embedding into Bert Embedding\n",
        "        \"\"\"\n",
        "        embedded = self.dropout(Get_Batch_Bert_Embeddings(input))\n",
        "        #embedded = self.dropout(self.embedding(input))\n",
        "        # embedded = [1, batch_size, emb_dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # output = [seq_len, batch_size, hid_dim * n_dir]\n",
        "        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_dir, batch_size, hid_dim]\n",
        "\n",
        "        # seq_len and n_dir will always be 1 in the decoder\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [batch_size, output_dim]\n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw8ZdRwED84p"
      },
      "source": [
        "### Seq2Seq\n",
        "For the final part of the implementation, we will implement the seq2seq model.\n",
        "\n",
        "- receive the input/source sentence\n",
        "\n",
        "- using the encoder to produce the context vectors\n",
        "\n",
        "- using the decoder to produce the predicted output / target sentence.\n",
        "\n",
        "Our full model will look like this:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/3a8dc5515ff28cb059532439c5687126dd30015f/assets/seq2seq4.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHBW6gOjD84p"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            'hidden dimensions of encoder and decoder must be equal.'\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            'n_layers of encoder and decoder must be equal.'\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src = [sen_len, batch_size]\n",
        "        # trg = [sen_len, batch_size]\n",
        "        # teacher_forcing_ratio : the probability to use the teacher forcing.\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <sos> token.\n",
        "        input = trg[0, :]\n",
        "        for t in range(1, trg_len):\n",
        "            # insert input token embedding, previous hidden and previous cell states\n",
        "            # receive output tensor (predictions) and new hidden and cell states.\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            # replace predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            # decide if we are going to use teacher forcing or not.\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # get the highest predicted token from our predictions.\n",
        "            top1 = output.argmax(1)\n",
        "            # update input : use ground_truth when teacher_force\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "BhfQde2WRKMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'device: {device}')\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqcUcbPMRJmy",
        "outputId": "bfd7c1ea-830d-48b7-be8b-1a2d39b0fd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n",
            "CUDA is not available.  Training on CPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71W93fR1D84r"
      },
      "source": [
        "## Training the Seq2Seq model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "k1reqKfORiYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ4RmEhOD84r"
      },
      "outputs": [],
      "source": [
        "# First initialize our model.\n",
        "\n",
        "import torch.cuda\n",
        "\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdbnImUKD84s",
        "outputId": "77d0d9c3-91ac-4565-b5de-2fa5d48d2b0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=5940, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YypGsm-KD84t",
        "outputId": "02d1f9dc-fb9d-4030-838d-e50731cb957d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 10,403,636 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH7fOwQuD84u"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xMXoTeLD84v"
      },
      "source": [
        "Next, we'll define our training loop.\n",
        "\n",
        "First. we'll set the model into \"training mode\" (turn on the dropout & batch normalization), and then iterate through our data iterator.\n",
        "\n",
        "As stated before, our decoder loop starts at 1, not 0. This means the 0th element of our outputs tensor remains all zeros. So our trg & outputs look something like:\n",
        "$$\n",
        "trg = [<sos>, y_1, y_2, y_3, <eos>]\n",
        "$$\n",
        "$$\n",
        "output = [0, \\hat{y}_1,\\hat{y}_2,\\hat{y}_3, <eos>]\n",
        "$$\n",
        "Here, when we calculate the loss, we cut off the first element of each tensor to get:\n",
        "$$\n",
        "trg = [y_1, y_2, y_3, <eos>]\n",
        "$$\n",
        "$$\n",
        "output = [\\hat{y}_1,\\hat{y}_2,\\hat{y}_3, <eos>]\n",
        "$$\n",
        "At each iterator:\n",
        "\n",
        "- get the source and target sentences from the batch, X and Y\n",
        "\n",
        "- zero the gradients calculated from the last batch\n",
        "\n",
        "- feed the source and target into the model to get the output $\\hat{y}$\n",
        "\n",
        "- as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with .view\n",
        "    - we slice off the first column of the output and target tensors as mentioned above.\n",
        "    \n",
        "- calculate the gradients with loss.backward()\n",
        "\n",
        "- clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
        "\n",
        "- update the parameters of our model by doing an optimizer.step()\n",
        "\n",
        "- sum the loss value to a running total.\n",
        "\n",
        "Finally, we return the loss that is average over all batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt4kjZzLD84v"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # trg = [sen_len, batch_size]\n",
        "        # output = [trg_len, batch_size, output_dim]\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        # trg = [(trg_len-1) * batch_size]\n",
        "        # output = [(trg_len-1) * batch_size, output_dim]\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mE416YQD84w"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) # turn off teacher forcing.\n",
        "\n",
        "            # trg = [sen_len, batch_size]\n",
        "            # output = [sen_len, batch_size, output_dim]\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwkHRWm5D84x"
      },
      "outputs": [],
      "source": [
        "# a function that used to tell us how long an epoch takes.\n",
        "def epoch_time(start_time, end_time):\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time  / 60)\n",
        "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
        "    return  elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4tPnc9tD84y"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'Seq2SeqModel.pt')\n",
        "    print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GiRzmcrD84y",
        "outputId": "735b0e77-c8ee-4dd4-f0b8-1bf963c81260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss : 4.053 | Test PPL:  57.566\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "    best_model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "    best_model.load_state_dict(torch.load('Seq2SeqModel.pt'))\n",
        "\n",
        "    test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "    print(f\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDIS1aOND84z"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Through the model is tranditional Seq2Seq model as the author mentioned before. As a beginner of pytorch and deeplearning, there is also many useful tricks worth learning. As a notebook I list them below:\n",
        "\n",
        "### Data Preparing Part\n",
        "\n",
        "- In the original paper, they find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\".\n",
        "\n",
        "it means when we want to get the context vector, it is beneficial to reverse the order of the input.\n",
        "\n",
        "- When build the vocabulary we can use min_freq parameter to remove the rare words in the corpus.\n",
        "\n",
        "e.g. SRC.build_vocab(train_data, min_freq=2)\n",
        "\n",
        "### Seq2Seq Model Part\n",
        "\n",
        "- In this notebook, we can learn how to build a deeplearning pipeline (seperate our model into different parts.) and combine them together.\n",
        "\n",
        "- It is a traditional Seq2Seq model, encoder is used to get the context vectors represented by the hidden and cell state generated by the last layer of LSTM. While decoder initialize its $h_0, c_0$ according to the encoder output. And each time-step $t$, generate the $t+1$ word in the target sentence. update the input according to the previous word and the teacher_force rate.\n",
        "\n",
        "- This is the first time I have ever seen the teacher_force rate. It is just a simple but useful way to restinct the changing of our model.\n",
        "\n",
        "### Train Part\n",
        "\n",
        "- Clip : as mentioned before, before we use optimizer.step(), we should use  torch.nn.utils.clip_grad_norm_(model.parameters(), clip) to avoiding gradients exploding ! Here we set clip=1."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}